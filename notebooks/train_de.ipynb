{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [{
          "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefangrotz/alman-alpaca/blob/main/notebooks/train_de.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on [alpaca lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py) and [train_lora.ipynb](https://github.com/stefangrotz/alman-alpaca/blob/main/notebooks/train_lora.ipynb)"
      ],
      "metadata": {
        "id": "UQF7nAH1syz4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXurA0q5jtaf"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget content/german_data.json https://raw.githubusercontent.com/stefangrotz/alman-alpaca/main/data/german_data.json"
      ],
      "metadata": {
        "id": "pJXj0ZZe-TFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, LlamaForCausalLM , LlamaTokenizer\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model"
      ],
      "metadata": {
        "id": "fhmLLJD0lM5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MICRO_BATCH_SIZE = 4  # this could actually be 5 but i like powers of 2\n",
        "BATCH_SIZE = 128\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "EPOCHS = 1  # we don't need 3 tbh\n",
        "LEARNING_RATE = 3e-4  # the Karpathy constant\n",
        "CUTOFF_LEN = 256  # 256 accounts for about 96% of the data\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05"
      ],
      "metadata": {
        "id": "XnTp0gOUlOCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('nlpcloud/instruct-gpt-j-fp16')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"nlpcloud/instruct-gpt-j-fp16\", load_in_8bit=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "vdQfvhHo0afo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = LLaMAForCausalLM.from_pretrained(\n",
        "#     \"decapoda-research/llama-7b-hf\",\n",
        "#     load_in_8bit=True,\n",
        "#     device_map=\"auto\",\n",
        "# )\n",
        "# tokenizer = LLaMATokenizer.from_pretrained(\n",
        "#     \"decapoda-research/llama-7b-hf\", add_eos_token=True\n",
        "# )\n",
        "\n",
        "model = prepare_model_for_int8_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "data = load_dataset(\"json\", data_files=\"/content/german_data.json\")"
      ],
      "metadata": {
        "id": "Xkb9pQTflS-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(data_point):\n",
        "    # desculpe o desastre de formatação, preciso ser rápido\n",
        "    if data_point[\"input\"]:\n",
        "        return f\"\"\"Abaixo está uma instrução que descreve uma tarefa, juntamente com uma entrada que fornece mais contexto. Escreva uma resposta que complete adequadamente o pedido.\n",
        "### Instrução:\n",
        "{data_point[\"instruction\"]}\n",
        "### Entrada:\n",
        "{data_point[\"input\"]}\n",
        "### Resposta:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Abaixo está uma instrução que descreve uma tarefa. Escreva uma resposta que complete adequadamente o pedido.\n",
        "### Instrução:\n",
        "{data_point[\"instruction\"]}\n",
        "### Resposta:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(prompt):\n",
        "    # there's probably a way to do this with the tokenizer settings\n",
        "    # but again, gotta move fast\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN + 1,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": result[\"input_ids\"][:-1],\n",
        "        \"attention_mask\": result[\"attention_mask\"][:-1],\n",
        "    }"
      ],
      "metadata": {
        "id": "_VCfL3BhlV_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.shuffle().map(lambda x: tokenize(generate_prompt(x)))"
      ],
      "metadata": {
        "id": "81oSm3GL9z72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=100,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=True,\n",
        "        logging_steps=20,\n",
        "        output_dir=\"lora-alpaca\",\n",
        "        save_total_limit=3,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train(resume_from_checkpoint=False)"
      ],
      "metadata": {
        "id": "INGJJZ6dkpJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora-cabrita-gptj\")"
      ],
      "metadata": {
        "id": "3JY4QEi7lXyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
        "\n",
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Abaixo está uma instrução que descreve uma tarefa, juntamente com uma entrada que fornece mais contexto. Escreva uma resposta que complete adequadamente o pedido.\n",
        "\n",
        "### Instrução:\n",
        "{instruction}\n",
        "\n",
        "### Entrada:\n",
        "{input}\n",
        "\n",
        "### Resposta:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Abaixo está uma instrução que descreve uma tarefa. Escreva uma resposta que complete adequadamente o pedido.\n",
        "\n",
        "### Instrução:\n",
        "{instruction}\n",
        "\n",
        "### Resposta:\"\"\"\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.2,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "def evaluate(instruction, input=None):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        print(\"Resposta:\", output.split(\"### Resposta:\")[1].strip())"
      ],
      "metadata": {
        "id": "lDSMOkOeJuos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(input(\"Instrução: \"))"
      ],
      "metadata": {
        "id": "Cl2dT9GCKpKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WrR0pQ6ZXIYN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
